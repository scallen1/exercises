---
title: "Assignment 2 Part 2"
author: "Stephen Allen"
date: "August 19, 2015"
output: html_document
---

We start this task by loading the neccessary libraries and reader function for our work.

```{r, results = "hide"}
library(tm)
library(randomForest)
library(e1071)
library(rpart)
library(ggplot2)
library(caret)
library(plyr)


readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), id=fname, language='en') }

```

Then we will load the training corpus.  Of note: we are setting a decently high bar of sparcity of the data due to the sheer size of the data involved.  Removing the sparsest terms helps the host computer's processor deal with this data in a reasonable time frame.

```{r}
author_dirs = Sys.glob('ReutersC50/C50train/*')
file_list = NULL
train_labels = NULL
for(author in author_dirs) {
  author_name = substring(author, first=23)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list = append(file_list, files_to_add)
  train_labels = append(train_labels, rep(author_name, length(files_to_add)))
}

all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))
train_corpus = Corpus(VectorSource(all_docs))
names(train_corpus) = file_list

train_corpus = tm_map(train_corpus, content_transformer(tolower)) 
train_corpus = tm_map(train_corpus, content_transformer(removeNumbers)) 
train_corpus = tm_map(train_corpus, content_transformer(removePunctuation)) 
train_corpus = tm_map(train_corpus, content_transformer(stripWhitespace)) 
train_corpus = tm_map(train_corpus, content_transformer(removeWords), stopwords("SMART"))

DTM_train = DocumentTermMatrix(train_corpus)
DTM_train = removeSparseTerms(DTM_train, 0.90)
```

We will do the same with the test data...

```{r, echo = FALSE}

author_dirs = Sys.glob('ReutersC50/C50test/*')
file_list = NULL
test_labels = NULL
for(author in author_dirs) {
  author_name = substring(author, first=22)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list = append(file_list, files_to_add)
  test_labels = append(test_labels, rep(author_name, length(files_to_add)))
}

all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))

test_corpus = Corpus(VectorSource(all_docs))
names(test_corpus) = file_list

test_corpus = tm_map(test_corpus, content_transformer(tolower)) 
test_corpus = tm_map(test_corpus, content_transformer(removeNumbers)) 
test_corpus = tm_map(test_corpus, content_transformer(removePunctuation)) 
test_corpus = tm_map(test_corpus, content_transformer(stripWhitespace)) 
test_corpus = tm_map(test_corpus, content_transformer(removeWords), stopwords("SMART"))


```

We also will use a dictionary to weed out terms that are in our test corpus but not our training corpus.  The model, based on the training data, is not prepared to deal with the test data.  From there we also weed out sparse terms from the test set and make these into dataframes.

```{r, results = "hide"}
reuters_dict = NULL
reuters_dict = dimnames(DTM_train)[[2]]

DTM_test = DocumentTermMatrix(test_corpus, list(dictionary=reuters_dict))
DTM_test = removeSparseTerms(DTM_test, 0.85)

```

``` {r, results = "hide"}
DTM_train_df = as.data.frame(inspect(DTM_train))
DTM_test_df = as.data.frame(inspect(DTM_test))
```

Then we model and predict with naiveBayes based on the the training data and applied to the test data and then plot the results.  As you can see, it is very clear that naive bayes does a very poor job of predictions, although the weak diagonal line suggests it is doing its job to some extent. 

```{r,fig.height=9, fig.width= 8}
model_NB = naiveBayes(x=DTM_train_df, y=as.factor(train_labels), laplace=1)

pred_NB = predict(model_NB, DTM_test_df)

table_NB = as.data.frame(table(pred_NB,test_labels))



plot = ggplot(table_NB)
plot + geom_tile(aes(x=test_labels, y=pred_NB, fill=Freq)) + 
  scale_x_discrete(name="True Author") + 
  scale_y_discrete(name="Predicted Author") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Next we will consider a method that I expected, and found, was much more successful in predicting the author of a given document.  Random Forest is modeled, used and graphed showing that it is a very effective predictive tool with a very sharply define diagonal line.

Random Forest is clearly preferred to Naive Bayes.

```{r, fig.height=9, fig.width=8}
DTM_test = as.matrix(DTM_test)
DTM_train = as.matrix(DTM_train)

xx <- data.frame(DTM_test[,intersect(colnames(DTM_test), colnames(DTM_train))])
yy <- read.table(textConnection(""), col.names = colnames(DTM_train), colClasses = "integer")

DTM_test_clean = rbind.fill(xx, yy)

DTM_test_df = as.data.frame(DTM_test_clean)


model_RF = randomForest(x=DTM_train_df, y=as.factor(train_labels), mtry=3, ntree=100)
pred_RF = predict(model_RF, data=DTM_test_clean)

table_RF = as.data.frame(table(pred_RF,test_labels))

plot = ggplot(table_RF)
plot + geom_tile(aes(x=test_labels, y=pred_RF, fill=Freq)) + 
  scale_x_discrete(name="True Author") + 
  scale_y_discrete(name="Predicted Author") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


